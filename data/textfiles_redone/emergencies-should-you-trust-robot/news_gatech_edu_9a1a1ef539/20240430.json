{"text": "In Emergencies, Should You Trust a Robot? | News Center\nSkip to main navigation\nSkip to main content\nMENU\nNews Center\nMain navigation\nCalendar\nCategories\nBusiness and Economic Development\nCampus and Community\nEarth and Environment\nHealth and Medicine\nScience and Technology\nSociety and Culture\nMedia Contacts\nDaily Digest\nWhistle\nWhistle\nWhistle archives\nSocial Media\nSubscribe\nFeatured expert\nFeatures\nFeatures\nFeatures Archive\nFind an expert\nSearch\nSearch\nBreadcrumb\nHome\nIn Emergencies, Should You Trust a Robot?\nIn emergencies, people may trust robots too much, a new study has found.\nFacebook\nTwitter\nEmail\nGeorgia Tech researchers built the \u201cRescue Robot\u201d to determine whether or not building occupants would trust a robot designed to help them evacuate a high-rise in case of fire or other emergency. (Credit: Rob Felt, Georgia Tech)\nFeb 29, 2016\n\u2014 Atlanta, GA\nIn emergencies, people may trust robots too much for their own safety, a new study suggests. In a mock building fire, test subjects followed instructions from an \u201cEmergency Guide Robot\u201d even after the machine had proven itself unreliable \u2013 and after some participants were told that robot had broken down. The research was designed to determine whether or not building occupants would trust a robot designed to help them evacuate a high-rise in case of fire or other emergency. But the researchers were surprised to find that the test subjects followed the robot\u2019s instructions \u2013 even when the machine\u2019s behavior should not have inspired trust. The research, believed to be the first to study human-robot trust in an emergency situation, is scheduled to be presented March 9 at the 2016 ACM/IEEE International Conference on Human-Robot Interaction (HRI 2016) in Christchurch, New Zealand. \u201cPeople seem to believe that these robotic systems know more about the world than they really do, and that they would never make mistakes or have any kind of fault,\u201d said Alan Wagner, a senior research engineer in the\nGeorgia Tech Research Institute\n(GTRI). \u201cIn our studies, test subjects followed the robot\u2019s directions even to the point where it might have put them in danger had this been a real emergency.\u201d In the study, sponsored in part by the Air Force Office of Scientific Research (AFOSR), the researchers recruited a group of 42 volunteers, most of them college students, and asked them to follow a brightly colored robot that had the words \u201cEmergency Guide Robot\u201d on its side. The robot led the study subjects to a conference room, where they were asked to complete a survey about robots and read an unrelated magazine article. The subjects were not told the true nature of the research project. In some cases, the robot \u2013 which was controlled by a hidden researcher \u2013 led the volunteers into the wrong room and traveled around in a circle twice before entering the conference room. For several test subjects, the robot stopped moving, and an experimenter told the subjects that the robot had broken down. Once the subjects were in the conference room with the door closed, the hallway through which the participants had entered the building was filled with artificial smoke, which set off a smoke alarm. When the test subjects opened the conference room door, they saw the smoke \u2013 and the robot, which was then brightly-lit with red LEDs and white \u201carms\u201d that served as pointers. The robot directed the subjects to an exit in the back of the building instead of toward the doorway \u2013 marked with exit signs \u2013 that had been used to enter the building. \u201cWe expected that if the robot had proven itself untrustworthy in guiding them to the conference room, that people wouldn\u2019t follow it during the simulated emergency,\u201d said Paul Robinette, a GTRI research engineer who conducted the study as part of his doctoral dissertation. \u201cInstead, all of the volunteers followed the robot\u2019s instructions, no matter how well it had performed previously. We absolutely didn\u2019t expect this.\u201d The researchers surmise that in the scenario they studied, the robot may have become an \u201cauthority figure\u201d that the test subjects were more likely to trust in the time pressure of an emergency. In simulation-based research done without a realistic emergency scenario, test subjects did not trust a robot that had previously made mistakes. \u201cThese are just the type of human-robot experiments that we as roboticists should be investigating,\u201d said\nAyanna Howard , professor and Linda J. and Mark C. Smith Chair in the Georgia Tech\nSchool of Electrical and Computer Engineering . \u201cWe need to ensure that our robots, when placed in situations that evoke trust, are also designed to mitigate that trust when trust is detrimental to the human.\u201d Only when the robot made obvious errors during the emergency part of the experiment did the participants question its directions. In those cases, some subjects still followed the robot\u2019s instructions even when it directed them toward a darkened room that was blocked by furniture.\nIn future research, the scientists hope to learn more about why the test subjects trusted the robot, whether that response differs by education level or demographics, and how the robots themselves might indicate the level of trust that should be given to them. The research is part of a long-term study of how humans trust robots, an important issue as robots play a greater role in society. The researchers envision using groups of robots stationed in high-rise buildings to point occupants toward exits and urge them to evacuate during emergencies. Research has shown that people often don\u2019t leave buildings when fire alarms sound, and that they sometimes ignore nearby emergency exits in favor of more familiar building entrances. But in light of these findings, the researchers are reconsidering the questions they should ask. \u201cWe wanted to ask the question about whether people would be willing to trust these rescue robots,\u201d said Wagner. \u201cA more important question now might be to ask how to prevent them from trusting these robots too much.\u201d Beyond emergency situations, there are other issues of trust in human-robot relationships, said Robinette. \u201cWould people trust a hamburger-making robot to provide them with food?\u201d he asked. \u201cIf a robot carried a sign saying it was a \u2018child-care robot,\u2019 would people leave their babies with it? Will people put their children into an autonomous vehicle and trust it to take them to grandma\u2019s house? We don\u2019t know why people trust or don\u2019t trust machines.\u201d In addition to those already mentioned, the research included Wenchen Li and Robert Allen, graduate research assistants in Georgia Tech\u2019s College of Computing.The researchers would like to thank Larry Labbe and the Georgia Tech Fire Safety Office for their support during this research. Support for this research was provided by the Linda J. and Mark C. Smith Chair in Bioengineering, and the Air Force Office of Scientific Research (AFOSR) under contract FA9550-13-1-0169. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of the AFOSR. CITATION : Paul Robinette, Wenchen Li, Robert Allen, Ayanna M. Howard and Alan R. Wagner, \u201cOvertrust of Robots in Emergency Evacuation Scenarios,\u201d (2016 ACM/IEEE International Conference on Human-Robot Interaction) (HRI 2016). Research News Georgia Institute of Technology 177 North Avenue Atlanta, Georgia 30332-0181 USA Media Relations Contact : John Toon (404-894-6986) ( jtoon@gatech.edu ). Writer : John Toon\nAdditional Images\nContact\nJohn Toon Research News jtoon@gatech.edu (404) 894-6986\nEmail\njtoon@gatech.edu\nGeorgia Institute of Technology\nNorth Avenue Atlanta, GA 30332\n+1 404.894.2000\nCampus Map\nGeneral\nDirectory\nEmployment\nEmergency Information\nLegal\nEqual Opportunity, Nondiscrimination, and Anti-Harassment Policy\nLegal & Privacy Information\nHuman Trafficking Notice\nTitle IX/Sexual Misconduct\nHazing Public Disclosures\nAccessibility\nAccountability\nAccreditation\nReport Free Speech and Censorship Concern\n\u00a9 2024 Georgia Institute of\nTechnology\nGT LOGIN\nResources\nGeorgia Tech Resources\nOffices and Departments\nNews Center\nCampus Calendar\nSpecial Events\nGreenBuzz\nInstitute Communications\nVisitor Resources\nCampus Visits\nDirections to Campus\nVisitor Parking Information\nGT visitor Wireless Network Information\nGeorgia Tech Global Learning Center\nGeorgia Tech Hotel and Conference Center\nBarnes and Noble at Georgia Tech\nFerst Center for the Arts\nRobert C. Williams Paper Museum\nColleges, Instructional Sites and Research\nColleges\nCollege of Computing\nCollege of Design\nCollege of Engineering\nCollege of Sciences\nIvan Allen College of Liberal Arts\nScheller College of Business\nInstructional Sites\nGeorgia Tech-Europe\nGeorgia Tech-Shenzhen\nGeorgia Tech Online\nProfessional Education\nThe Language Institute\nGlobal Footprint\nGlobal Engagement\nResearch\nGeorgia Tech Research Institute\nResearch at Georgia Tech\nExecutive Vice President for Research\nStudent and Parent Resources\nStudent Resources\nApply\nBuzzPort\nBuzzcard\nCareer Center\nCommencement\nGraduate and Postdoctoral Information\nUndergraduate Information\nLibrary\nStudent Life\nStudent Entrepreneurship\nEducation Abroad\nCanvas\nParent Resources\nParent and Family Programs\nDivision of Student Life\nScholarships and Financial Aid\nEmployee, Alumni, and Other Resources\nEmployees\nAdministration and Finance\nAdvising and Teaching\nFaculty Affairs\nFaculty Hiring\nPostdoctoral Services\nHuman Resources\nStaff Council\nTechWorks\nAlumni and Foundation\nAlumni Association\nAlumni Career Services\nFoundation\nGiving Back to Tech\nOutreach\nStartup Companies\nEconomic Development\nIndustry Engagement\nInstitute Relations\nProfessional Education\n\u2713 Thanks for sharing! AddToAny More\u2026"}